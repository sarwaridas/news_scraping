#   retrieve_urls.py#==========================================================================="""This will take concatenated words of water issue (notice.txt, location listed (state.txt) in notice.txt, and URLs (urls.txt). It will then take these combinations and query Google for grey literature. Next, it will take the websites that are produced from this search and generate a spreadsheet of sites."""#===========================================================================#   Import the modules hereimport csvimport osimport time#   See search.pyimport search#   How many seconds to wait after an errorexception_delay = 100#   How many seconds to wait between searchesbetween_pages_delay = 60.0#   The most URLs to collect for any given set of search termsgoal_urls = 200#   This generates a log of the searches that have been completed.log_path = "previous_searches.log"if os.path.exists(log_path):    with open(log_path, "r") as log_file:        previous = set([line.strip() for line in log_file])else:    previous = set()log = open(log_path, "a+")#   This read in list of locations (within states) #   for building querieswith open("notice.txt", "r") as notice_file:    nots = [line.strip() for line in notice_file]#   This reads in list of states for building querieswith open("state.txt", "r") as state_file:    states = [line.strip() for line in state_file]#   This reads in list of water keywords for building querieswith open("urls.txt", "r") as site_file:    sites = [line.strip() for line in site_file]notice_path = "noticeURL.csv"#   This is the output file for all queries and resultsadding = os.path.exists(notice_path)#   All the URLs for all entities and shocks that have been seen so far#   If this search is new, start from scratch, otherwise read in the previous findsif adding:    with open(notice_path, "r") as previous_notices:        all_urls = []        reader = csv.reader(previous_notices,                             delimiter=',',                            quotechar='"',                            quoting=csv.QUOTE_MINIMAL)        # Skip header        next(reader)        for row in reader:            if len(row) > 0:                all_urls.append(row[-1])else:    all_urls = []notice_out = open(notice_path, "a+", newline='')csv_writer = csv.writer(notice_out,                        delimiter=",",                        quotechar='"',                        quoting=csv.QUOTE_MINIMAL)#   Column headers to reproduce search terms and resultsif not adding:    csv_writer.writerow(["notice",                         "state",                         "URL"])#   For keeping track of total progress through combinationsnum_combos = len(nots) * len(states) * len(sites)#   For keeping track of which query we're onprogress = 0print("Starting " + str(num_combos) + " searches...")if len(previous) > 0:    print("Skipping " + str(len(previous)) + " already completed searches...")#   Iterate through all combinations of search terms (by category)for notice in nots:    for state in states:        for site in sites:                #   For keeping track of which query we're on                progress += 1                #   This concatenates term (term) and URL (url)                query = " ".join([notice, state, site])                if query in previous:                    continue                print(str(progress) + "/" + str(num_combos) + "\t" + query)                #   Results for this query go here                urls = []                #   Finished is true iff we have reached the end of the search                 #   results for this query                finished = False                #   How many search results found for this query (incl.                 #   duplicates)                num_results = 0                #   Iterate through search results for this query                #   stop if we have found at least goal_urls new URLs or                #   we're at the end of the results pages                while len(urls) < goal_urls and not finished:                    results = []                    #   How many search results per page                    units = 10                    #   How many attempts at the search have we failed?                    attempts = 0                    #   503 errors are likely, so we will try three times to                     #   connect for each query                    #   Important: if 3 failures, no results will be recorded,                     #   but the next query will be tried                    while attempts < 3:                        try:                            #   Starting at index 0 of search results and                             #   getting the first `units` results, then the                             #   second `units` results, etc.                            user_agent = search.get_random_user_agent()                            referer = search.get_random_referer()                            results = search.search(query,                                                    num=units,                                                    start=num_results,                                                    stop=(num_results+units),                                                    pause=between_pages_delay,                                                    user_agent=user_agent,                                                    referer=referer)                            #   Generator to array                            results = [r for r in results]                            #   Keep track of inconsistent number of results                            num_results = num_results + len(results)                            #   No need to continue if there are no more results to be had                            if len(results) < (units - 1):                                finished = True                            #   Filter out URLs we've already encountered (for                             #   any search)                            results = [r for r in results if not r in all_urls]                            #   We succeeded, so no need for more attempts                            break                        except Exception as e:                            print(e)                            finished = True                            attempts += 1                            print("failed " + str(attempts) + " attempts to search")                            time.sleep(exception_delay)                    #   Add new URLs to results for this query                    urls = urls + results                    print("%i new results, %i total" % (len(results), len(urls)))                #   Maximum goal_urls URLS for each query                urls = urls[0:goal_urls]                #   Add current results to URLs seen so far so we don't                #   replicate them with later searches                all_urls = all_urls + urls                #   Print current query and search results to CSV                                for url in urls:                    csv_writer.writerow([notice,                                          state,                                           site,                                         url])                log.write(query + "\n")notice_out.close()log.close()search.browser.quit()